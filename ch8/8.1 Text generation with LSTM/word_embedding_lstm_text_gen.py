"""
åŸºäºè¯åµŒå…¥çš„ LSTM æ–‡æœ¬ç”Ÿæˆ
"""

import random
import tensorflow as tf
from tensorflow.keras import optimizers
from tensorflow.keras import layers
from tensorflow.keras import models
from tensorflow import keras
import numpy as np
import jieba
import os

os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"

path = '/Users/c/Desktop/b.txt'
text = open(path).read().lower().replace('\n', '').replace('ã€€ã€€', '\n')
print('Corpus length:', len(text))

# å°†æ–‡æœ¬åºåˆ—å‘é‡åŒ–

maxlen = 60     # æ¯ä¸ªåºåˆ—çš„é•¿åº¦
step = 3        # æ¯ 3 ä¸ª token é‡‡æ ·ä¸€ä¸ªæ–°åºåˆ—
sentences = []  # ä¿å­˜æ‰€æå–çš„åºåˆ—
next_tokens = []  # sentences çš„ä¸‹ä¸€ä¸ª token

print('Vectorization...')

token_text = list(jieba.cut(text))

tokens = list(set(token_text))
tokens_indices = {token: tokens.index(token) for token in tokens}
print('Number of tokens:', len(tokens))

for i in range(0, len(token_text) - maxlen, step):
    sentences.append(
        list(map(lambda t: tokens_indices[t], token_text[i: i+maxlen])))
    next_tokens.append(tokens_indices[token_text[i+maxlen]])
print('Number of sequences:', len(sentences))

next_tokens_one_hot = []
for i in next_tokens:
    y = np.zeros((len(tokens),), dtype=np.bool)
    y[i] = 1
    next_tokens_one_hot.append(y)

dataset = tf.data.Dataset.from_tensor_slices((sentences, next_tokens_one_hot))
dataset = dataset.shuffle(buffer_size=4096)
dataset = dataset.batch(128)
dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)


# æ„å»ºæ¨¡å‹

model = models.Sequential([
    layers.Embedding(len(tokens), 256),
    layers.LSTM(256),
    layers.Dense(len(tokens), activation='softmax')
])

# æ¨¡å‹ç¼–è¯‘é…ç½®

optimizer = optimizers.RMSprop(lr=0.1)
model.compile(loss='categorical_crossentropy',
              optimizer=optimizer)

# é‡‡æ ·å‡½æ•°


def sample(preds, temperature=1.0):
    '''
    å¯¹æ¨¡å‹å¾—åˆ°çš„åŸå§‹æ¦‚ç‡åˆ†å¸ƒé‡æ–°åŠ æƒï¼Œå¹¶ä»ä¸­æŠ½å–ä¸€ä¸ª token ç´¢å¼•
    '''
    preds = np.asarray(preds).astype('float64')
    preds = np.log(preds) / temperature
    exp_preds = np.exp(preds)
    preds = exp_preds / np.sum(exp_preds)
    probas = np.random.multinomial(1, preds, 1)
    return np.argmax(probas)

# è®­ç»ƒæ¨¡å‹


callbacks_list = [
    # åœ¨æ¯è½®å®Œæˆåä¿å­˜æƒé‡
    keras.callbacks.ModelCheckpoint(
        filepath='text_gen.h5',  # ä¿å­˜æ–‡ä»¶çš„è·¯å¾„
        monitor='loss',      # monitorï¼šè¦éªŒè¯çš„æŒ‡æ ‡
        save_best_only=True,     # åªä¿å­˜è®© monitor æŒ‡æ ‡æœ€å¥½çš„æ¨¡å‹ï¼ˆå¦‚æœ monitor æ²¡æœ‰æ”¹å–„ï¼Œå°±ä¸ä¿å­˜ï¼‰
    ),
    # ä¸å†æ”¹å–„æ—¶é™ä½å­¦ä¹ ç‡
    keras.callbacks.ReduceLROnPlateau(
        monitor='loss',    # è¦éªŒè¯çš„æŒ‡æ ‡
        factor=0.5,            # è§¦å‘æ—¶ï¼šå­¦ä¹ ç‡ *= factor
        patience=1,            # monitor åœ¨ patience è½®å†…æ²¡æœ‰æ”¹å–„ï¼Œåˆ™è§¦å‘é™ä½å­¦ä¹ ç‡
    ),
    # ä¸å†æ”¹å–„æ—¶ä¸­æ–­è®­ç»ƒ
    keras.callbacks.EarlyStopping(
        monitor='loss',           # è¦éªŒè¯çš„æŒ‡æ ‡
        patience=3,             # å¦‚æœ monitor åœ¨å¤šäº patience è½®å†…ï¼ˆæ¯”å¦‚è¿™é‡Œå°±æ˜¯10+1=11è½®ï¼‰æ²¡æœ‰æ”¹å–„ï¼Œåˆ™ä¸­æ–­è®­ç»ƒ
    ),
]

model.fit(dataset, epochs=60, callbacks=callbacks_list)

# æ–‡æœ¬ç”Ÿæˆ

start_index = random.randint(0, len(text) - maxlen - 1)
generated_text = text[start_index: start_index + maxlen]
# print(f' Generating with seed: "{generated_text}"')
print(f'  ğŸ“– Generating with seed: "\033[1;32;43m{generated_text}\033[0m"')

for temperature in [0.2, 0.5, 1.0, 1.2]:
    # print('\n  temperature:', temperature)
    print(f'\n   \033[1;36m ğŸŒ¡ï¸ temperature: {temperature}\033[0m')
    print(generated_text, end='')
    for i in range(400):    # ç”Ÿæˆ 400 ä¸ª token
        # ç¼–ç å½“å‰æ–‡æœ¬
        text_cut = jieba.cut(generated_text)
        sampled = []
        for i in text_cut:
            if i in tokens_indices:
                sampled.append(tokens_indices[i])
            else:
                sampled.append(0)

        # é¢„æµ‹ï¼Œé‡‡æ ·ï¼Œç”Ÿæˆä¸‹ä¸€ä¸ª token
        preds = model.predict(sampled, verbose=0)[0]
        next_index = sample(preds, temperature)
        next_token = tokens[next_index]
        print(next_token, end='')

        generated_text = generated_text[1:] + next_token
