{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Python\n",
    "\n",
    "## 6.1  Working with text data\n",
    "\n",
    "> å¤„ç†æ–‡æœ¬æ•°æ®\n",
    "\n",
    "è¦ç”¨æ·±åº¦å­¦ä¹ çš„ç¥ç»ç½‘ç»œå¤„ç†æ–‡æœ¬æ•°æ®ï¼Œå’Œå›¾ç‰‡ç±»ä¼¼ï¼Œä¹Ÿè¦æŠŠæ•°æ®å‘é‡åŒ–ï¼šæ–‡æœ¬ -> æ•°å€¼å¼ é‡ã€‚\n",
    "\n",
    "è¦åšè¿™ç§äº‹æƒ…å¯ä»¥æŠŠæ¯ä¸ªå•è¯å˜æˆå‘é‡ï¼Œä¹Ÿå¯ä»¥æŠŠå­—ç¬¦å˜æˆå‘é‡ï¼Œè¿˜å¯ä»¥æŠŠå¤šä¸ªè¿ç»­å•è¯æˆ–å­—ç¬¦(ç§°ä¸º *N-grams*)å˜æˆå‘é‡ã€‚\n",
    "\n",
    "åæ­£ä¸ç®¡å¦‚ä½•åˆ’åˆ†ï¼Œæˆ‘ä»¬æŠŠæ–‡æœ¬æ‹†åˆ†å‡ºæ¥çš„å•å…ƒå«åš *tokens*ï¼ˆæ ‡è®°ï¼‰ï¼Œæ‹†åˆ†æ–‡æœ¬çš„è¿‡ç¨‹å«åš *tokenization*(åˆ†è¯)ã€‚\n",
    "\n",
    "> æ³¨ï¼štoken çš„ä¸­æ–‡ç¿»è¯‘æ˜¯â€œæ ‡è®°â€ğŸ˜‚ã€‚è¿™äº›ç¿»è¯‘éƒ½æ€ªæ€ªçš„ï¼Œè™½ç„¶ token ç¡®å®æœ‰æ ‡è®°è¿™ä¸ªæ„æ€ï¼Œä½†æŠŠè¿™é‡Œçš„ token ç¿»è¯‘æˆæ ‡è®°å°±æ²¡å†…å‘³å„¿äº†ã€‚æˆ‘è§‰å¾— token æ˜¯é‚£ç§ä»¥ä¸€ä¸ªä¸œè¥¿ä»£è¡¨å¦ä¸€ä¸ªä¸œè¥¿æ¥ä½¿ç”¨çš„æ„æ€ï¼Œè¿™ç§ token æ˜¯ä¸€ç§æœ‰å®ä½“çš„ä¸œè¥¿ï¼Œæ¯”å¦‚ä»£é‡‘åˆ¸ã€‚â€œæ ‡è®°â€è¿™ä¸ªè¯åœ¨å­—å…¸ä¸Šä½œåè¯æ˜¯ã€Œèµ·æ ‡ç¤ºä½œç”¨çš„è®°å·ã€çš„æ„æ€ï¼Œè€Œæˆ‘è§‰å¾—è®°å·ä¸æ˜¯ä¸ªå¾ˆå®ä½“çš„ä¸œè¥¿ã€‚ä»£é‡‘åˆ¸ä¸æ˜¯ä¸€ç§è®°å·ã€ä¹Ÿå°±èƒ½è¯´æ˜¯æ ‡è®°ï¼ŒåŒæ ·çš„ï¼Œè¿™é‡Œçš„ token ä¹Ÿæ˜¯ä¸€ç§å®ä½“çš„ä¸œè¥¿ï¼Œæˆ‘è§‰å¾—ä¸èƒ½æŠŠå®ƒè¯´æˆæ˜¯â€œæ ‡è®°â€ã€‚æˆ‘ä¸èµåŒè¿™ç§è¯‘æ³•ï¼Œæ‰€ä»¥ä¸‹æ–‡æ‰€æœ‰æ¶‰åŠ token çš„åœ°æ–¹ç»Ÿä¸€å†™æˆ â€œtokenâ€ï¼Œä¸ç¿»è¯‘æˆâ€œæ ‡è®°â€ã€‚\n",
    "\n",
    "\n",
    "æ–‡æœ¬çš„å‘é‡åŒ–å°±æ˜¯å…ˆä½œåˆ†è¯ï¼Œç„¶åæŠŠç”Ÿæˆå‡ºæ¥çš„ token é€ä¸ªä¸æ•°å€¼å‘é‡å¯¹åº”èµ·æ¥ï¼Œæœ€åæ‹¿å¯¹åº”çš„æ•°å€¼å‘é‡åˆæˆä¸€ä¸ªè¡¨è¾¾äº†åŸæ–‡æœ¬çš„å¼ é‡ã€‚å…¶ä¸­ï¼Œæ¯”è¾ƒæœ‰æ„æ€çš„æ˜¯å¦‚ä½•å»ºç«‹ token å’Œ æ•°å€¼å‘é‡ çš„è”ç³»ï¼Œä¸‹é¢ä»‹ç»ä¸¤ç§æè¿™ä¸ªçš„æ–¹æ³•ï¼šone-hot encoding(one-hotç¼–ç ) å’Œ token embedding(æ ‡è®°åµŒå…¥)ï¼Œå…¶ä¸­ token embedding ä¸€èˆ¬éƒ½ç”¨äºå•è¯ï¼Œå«ä½œè¯åµŒå…¥ã€Œword embeddingã€ã€‚\n",
    "\n",
    "![æ–‡æœ¬çš„å‘é‡åŒ–ï¼šä»æ–‡æœ¬åˆ°tokenå†åˆ°å¼ é‡](https://tva1.sinaimg.cn/large/007S8ZIlgy1ghek3mhp38j31320mg0v8.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-grams å’Œè¯è¢‹(bag-of-words)\n",
    "\n",
    "n-gram æ˜¯èƒ½ä»ä¸€ä¸ªå¥å­ä¸­æå–å‡ºçš„ â‰¤N ä¸ªè¿ç»­å•è¯çš„é›†åˆã€‚ä¾‹å¦‚ï¼šã€ŒThe cat sat on the mat.ã€\n",
    "\n",
    "è¿™ä¸ªå¥å­åˆ†è§£æˆ 2-gram æ˜¯ï¼š\n",
    "\n",
    "```\n",
    "{\"The\", \"The cat\", \"cat\", \"cat sat\", \"sat\",\n",
    "  \"sat on\", \"on\", \"on the\", \"the\", \"the mat\", \"mat\"}\n",
    "```\n",
    "\n",
    "è¿™ä¸ªé›†åˆè¢«å«åš bag-of-2-grams (äºŒå…ƒè¯­æ³•è¢‹)ã€‚\n",
    "\n",
    "åˆ†è§£æˆ 3-gram æ˜¯ï¼š\n",
    "\n",
    "```\n",
    "{\"The\", \"The cat\", \"cat\", \"cat sat\", \"The cat sat\",\n",
    "  \"sat\", \"sat on\", \"on\", \"cat sat on\", \"on the\", \"the\",\n",
    "  \"sat on the\", \"the mat\", \"mat\", \"on the mat\"}\n",
    "```\n",
    "\n",
    "è¿™ä¸ªé›†åˆè¢«å«åš bag-of-3-grams (ä¸‰å…ƒè¯­æ³•è¢‹)ã€‚\n",
    "\n",
    "æŠŠè¿™ä¸œè¥¿å«åšã€Œè¢‹ã€æ˜¯å› ä¸ºå®ƒåªæ˜¯ tokens ç»„æˆçš„é›†åˆï¼Œæ²¡æœ‰åŸæ¥æ–‡æœ¬çš„é¡ºåºå’Œæ„ä¹‰ã€‚æŠŠæ–‡æœ¬åˆ†æˆè¿™ç§è¢‹çš„åˆ†è¯æ–¹æ³•å«åšã€Œè¯è¢‹(bag-of-words)ã€ã€‚\n",
    "\n",
    "ç”±äºè¯è¢‹æ˜¯ä¸ä¿å­˜é¡ºåºçš„ï¼ˆåˆ†å‡ºæ¥æ˜¯é›†åˆï¼Œä¸æ˜¯åºåˆ—ï¼‰ï¼Œæ‰€ä»¥ä¸€èˆ¬ä¸åœ¨æ·±åº¦å­¦ä¹ é‡Œé¢ç”¨ã€‚ä½†åœ¨è½»é‡çº§çš„æµ…å±‚æ–‡æœ¬å¤„ç†æ¨¡å‹é‡Œé¢ï¼Œn-gram å’Œè¯è¢‹è¿˜æ˜¯å¾ˆé‡è¦çš„æ–¹æ³•çš„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot ç¼–ç \n",
    "\n",
    "one-hot æ˜¯æ¯”è¾ƒåŸºæœ¬ã€å¸¸ç”¨çš„ã€‚å…¶åšæ³•æ˜¯å°†æ¯ä¸ª token ä¸ä¸€ä¸ªå”¯ä¸€æ•´æ•°ç´¢å¼•å…³è”ï¼Œ ç„¶åå°†æ•´æ•°ç´¢å¼• i è½¬æ¢ä¸ºé•¿åº¦ä¸º N çš„äºŒè¿›åˆ¶å‘é‡(N æ˜¯è¯è¡¨å¤§å°)ï¼Œè¿™ä¸ªå‘é‡åªæœ‰ç¬¬ i ä¸ªå…ƒç´ ä¸º 1ï¼Œå…¶ä½™å…ƒç´ éƒ½ä¸º 0ã€‚\n",
    "\n",
    "ä¸‹é¢ç»™å‡ºç©å…·ç‰ˆæœ¬çš„ one-hot ç¼–ç ç¤ºä¾‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# å•è¯çº§çš„ one-hot ç¼–ç \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "token_index = {}\n",
    "for sample in samples:\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) + 1\n",
    "            \n",
    "# å¯¹æ ·æœ¬è¿›è¡Œåˆ†è¯ã€‚åªè€ƒè™‘æ¯ä¸ªæ ·æœ¬å‰ max_length ä¸ªå•è¯\n",
    "max_length = 10\n",
    "\n",
    "results = np.zeros(shape=(len(samples), \n",
    "                          max_length, \n",
    "                          max(token_index.values()) + 1))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i, j, index] = 1.\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  [1. 1. 1. ... 1. 1. 1.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# å­—ç¬¦çº§çš„ one-hot ç¼–ç \n",
    "\n",
    "import string\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "characters = string.printable    # æ‰€æœ‰å¯æ‰“å°çš„ ASCII å­—ç¬¦\n",
    "token_index = dict(zip(range(1, len(characters) + 1), characters))\n",
    "\n",
    "max_length = 50\n",
    "results = np.zeros((len(samples), max_length, max(token_index.keys()) + 1))\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, character in enumerate(sample):\n",
    "        index = token_index.get(character)\n",
    "        results[i, j, index] = 1.\n",
    "        \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras å†…ç½®äº†æ¯”åˆšæ‰å†™çš„è¿™ç§ç©å…·ç‰ˆæœ¬å¼ºå¤§å¾—å¤šçš„ one-hot ç¼–ç å·¥å…·ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 unique tokens.\n",
      "[[0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n",
      "{'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000)    # åªè€ƒè™‘å‰ 1000 ä¸ªæœ€å¸¸è§çš„å•è¯\n",
    "tokenizer.fit_on_texts(samples)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(samples)    # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•´æ•°ç´¢å¼•ç»„æˆçš„åˆ—è¡¨\n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "\n",
    "word_index = tokenizer.word_index    # å•è¯ç´¢å¼•ï¼Œå°±æ˜¯è¯è¡¨å­—å…¸å•¦ï¼Œç”¨è¿™ä¸ªå°±å¯ä»¥è¿˜åŸæ•°æ®\n",
    "\n",
    "print(f'Found {len(word_index)} unique tokens.')\n",
    "print(one_hot_results, word_index, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
